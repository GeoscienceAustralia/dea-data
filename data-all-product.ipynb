{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 STAT Jupyter notebook for all products avaulable on DEA public bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import os.path\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "import re\n",
    "from shapely.geometry import Polygon\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE = 'data.csv'\n",
    "S3_INPUT_BUCKET = 's3stat-monitoring'\n",
    "S3_OUTPUT_BUCKET = 'dea-public-data-dev'\n",
    "\n",
    "# Remove output file if it exists\n",
    "if Path(CSV_FILE).is_file():\n",
    "    Path(CSV_FILE).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(reader):\n",
    "    '''\n",
    "    Read the json input file\n",
    "    '''\n",
    "    reader_files = reader['Files']\n",
    "    today = date.today()  \n",
    "    results = []\n",
    "    for i,v in reader_files.items():\n",
    "        if str(i).endswith('.TIF') or str(i).endswith('.tif') or str(i).endswith('.tiff'):\n",
    "            results.append({\"folder\": str(i), \"hits\": int(v[0]), \"bytes\": int(v[1]), 'date':today})\n",
    "    return results\n",
    "\n",
    "def tile_index_from_path(path):\n",
    "    def go(part):\n",
    "        pattern = r'(?P<key>[xy])_(?P<num>.*)'\n",
    "        match = re.match(pattern, part, re.IGNORECASE)\n",
    "        if match is None:\n",
    "            return {}\n",
    "        gd = match.groupdict()\n",
    "        return {gd['key']: gd['num']}\n",
    "    \n",
    "    result = {}\n",
    "    for part in path.split('/'):\n",
    "        result.update(go(part))\n",
    "    return ((result['x']), (result['y']))\n",
    "\n",
    "def product_name(folder):\n",
    "    parts = Path(folder).parts\n",
    "    if parts[0] == 'mangrove_cover':\n",
    "        return parts[0]\n",
    "    return os.path.join(*parts[:2])\n",
    "\n",
    "def spatial_id(folder):\n",
    "    parts = Path(folder).parts\n",
    "    if parts[-2] in ['NBAR', 'NBART', 'QA', 'SUPPLEMENTARY', 'LAMBERTIAN']:\n",
    "        pass\n",
    "    if len(parts) > 2 and parts[0] == 'L2' and parts[1] == 'sentinel-2-nrt' and parts[-2] in ['NBAR', 'NBART', 'QA', 'SUPPLEMENTARY', 'LAMBERTIAN']:\n",
    "        try:\n",
    "            return parts[-3].split(\"_\")[-2][1:]\n",
    "        except IndexError:\n",
    "            print(folder)\n",
    "    if len(parts) >2 and parts[0] == 'hltc' or parts[0] == 'item_v2':\n",
    "        try:\n",
    "            return parts[-1].split(\"_\")[2]\n",
    "        except IndexError:\n",
    "            print(folder)\n",
    "    if len(parts) >2 and parts[0] == 'nidem':\n",
    "        try:\n",
    "            return parts[-1].split(\"_\")[1]\n",
    "        except IndexError:\n",
    "            print(folder)\n",
    "    elif len(parts) >2 and parts[0]=='bare-earth' or parts[0]=='geomedian-australia' or parts[0]=='WOfS' or parts[0]=='fractional-cover':\n",
    "        return ','.join(tile_index_from_path(folder))\n",
    "    elif len(parts) >2 and parts[0] == 'projects' or parts[0] == 'weathering-intensity':\n",
    "        return ' '\n",
    "    elif len(parts) >2 and parts[0] == 'multi-scale-topographic-position':\n",
    "        return ' '\n",
    "    else:\n",
    "        return '<none>'\n",
    "\n",
    "def coordsgeojson(spatialid):\n",
    "    with open('australian-mgrs-tiles.geojson') as fl:\n",
    "        input_gj = json.load(fl)\n",
    "    feats = input_gj['features']\n",
    "    for feat in feats:\n",
    "        if 'MGRS' in feat['properties']:\n",
    "            mgrs = feat['properties']['MGRS']\n",
    "            if spatial_id == mgrs:\n",
    "                polygon = Polygon(feat[\"geometry\"]['coordinates'][0])\n",
    "                lat = round(polygon.centroid.y, 1)\n",
    "                lon = round(polygon.centroid.x, 1) \n",
    "                return lat\n",
    "    \n",
    "  \n",
    "    \n",
    "def latcord(folder, spatialid):\n",
    "    parts = Path(folder).parts\n",
    "\n",
    "    if len(parts) >2 and parts[0] == 'hltc' or parts[0] == 'item_v2':\n",
    "        return (parts[-1].split('.tif'))[0].split(\"_\")[4]\n",
    "    elif len(parts) >2 and parts[0] == 'nidem':\n",
    "        return parts[-1].split(\"_\")[3]\n",
    "    elif len(parts) >2 and parts[0] == 'projects' or parts[0] == 'weathering-intensity':\n",
    "        return ' '\n",
    "    elif len(parts) >2 and parts[0] == 'multi-scale-topographic-position':\n",
    "        return ' '\n",
    "    else:\n",
    "        return '<none>'\n",
    "\n",
    "def loncord(folder, spatialid):\n",
    "    parts = Path(folder).parts\n",
    "    if len(parts) >2 and parts[0] == 'hltc' or parts[0] == 'item_v2':\n",
    "        return parts[-1].split(\"_\")[3]\n",
    "    elif len(parts) >2 and parts[0] == 'nidem':\n",
    "        return parts[-1].split(\"_\")[2]\n",
    "    elif len(parts) >2 and parts[0] == 'projects' or parts[0] == 'weathering-intensity':\n",
    "        return ' '\n",
    "    elif len(parts) >2 and parts[0] == 'multi-scale-topographic-position':\n",
    "        return ' '\n",
    "    else:\n",
    "        return '<none>'\n",
    "\n",
    "def merge_pre(folder_name, dicts, file_date):\n",
    "    dt = datetime.strptime(file_date + '01', \"%Y%m%d\")\n",
    "    return {\n",
    "        'date': dt.strftime(\"%A, %d-%B-%Y\"),\n",
    "        'product': product_name(folder_name),\n",
    "        'spatial_id': spatial_id(folder_name),\n",
    "        'Lat' : latcord(folder_name, spatial_id(folder_name)),\n",
    "        'Lon' : loncord(folder_name, spatial_id(folder_name)),\n",
    "        'hits': max(int(d['hits']) for d in dicts),\n",
    "        'bytes/GB': round(sum(int(d['bytes']) for d in dicts)/1000000000,2),\n",
    "        'folder': folder_name\n",
    "    }\n",
    "\n",
    "def group(entry_list, key):\n",
    "    lookup = defaultdict(list)\n",
    "    \n",
    "    for d in entry_list:\n",
    "        lookup[d[key]].append(d)\n",
    "        \n",
    "    return lookup\n",
    "\n",
    "def get_monthly_jsons(s3_client):\n",
    "    every =  [entry['Key']\n",
    "              for entry in s3_client.list_objects(Bucket=S3_INPUT_BUCKET)['Contents']\n",
    "              if entry['Key'].startswith('stats/month')]\n",
    "    \n",
    "    assert len(every) >= 2, \"Not enough monthly .json files to pick the last completed one\"\n",
    "    return every[:-1]\n",
    "\n",
    "def stats(monthly_json, s3_client, features):\n",
    "    json_body = read_json(json.loads(s3_client.get_object(Bucket=S3_INPUT_BUCKET, Key=monthly_json)['Body'].read().decode('utf-8')))\n",
    "    file_date = monthly_json.split('.')[0].split('/')[2]\n",
    "    stage2 = [merge_pre(key, value, file_date) for key, value in group(json_body, 'folder').items()]\n",
    "\n",
    "    products = [d for d in stage2]\n",
    "    for feat in features:\n",
    "        if 'label' in feat['properties']:\n",
    "            label = feat['properties']['label']\n",
    "        elif 'MGRS' in feat['properties']:\n",
    "            label = feat['properties']['MGRS']\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "        for dict_item in products:\n",
    "            if dict_item['spatial_id'] == label:\n",
    "                polygon = Polygon(feat[\"geometry\"]['coordinates'][0]) \n",
    "                dict_item['Lat'] = round(polygon.centroid.y, 1)\n",
    "                dict_item['Lon'] = round(polygon.centroid.x, 1)     \n",
    "\n",
    "    with open(CSV_FILE, 'a+') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, list(stage2[0]))\n",
    "        if Path(CSV_FILE).stat().st_size == 0:\n",
    "            dict_writer.writeheader()  # file doesn't exist yet, write a header\n",
    "        else:\n",
    "            dict_writer.writerows(products)\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name='devProfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = get_monthly_jsons(s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the tabs from top to above here and if you are writing out to a file for first time  use the below cell with `w` marker to write the headers on top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MGRS tiles features    \n",
    "with open('australian-mgrs-tiles.geojson') as fl:\n",
    "    mgrs_features = json.load(fl)['features']\n",
    "\n",
    "# Extract albers grid features    \n",
    "with open('albers_grid.geojson') as fl:\n",
    "    albers_features = json.load(fl)['features']\n",
    "\n",
    "# Loop through files within s3stat-monitoring/stats/month bucket and process monthly\n",
    "for monthly_json in jsons:\n",
    "    stats(monthly_json, s3_client, albers_features + mgrs_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the AWS profile for dev envronment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CSV_FILE, 'rb') as newdata:\n",
    "    # Upload a copy of the output csv file\n",
    "    s3_client.put_object(Bucket=S3_OUTPUT_BUCKET, Key=f's3-csv/data_{str(datetime.now().strftime(\"%Y%m%d_%H:%M:%S\"))}.csv', Body=newdata)\n",
    "    \n",
    "    # Update the actual csv file\n",
    "    s3_client.put_object(Bucket=S3_OUTPUT_BUCKET, Key=f's3-csv/data.csv', Body=newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
